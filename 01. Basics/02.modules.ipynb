{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Modules\n",
    "Modules are the basic building blocks of neural networks. They are the components that can be combined to create a neural network. A module can be a layer, an activation function, a loss function, etc. In this notebook, we will learn how to create modules and how to use them to create a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5) # Learn more about this in other sections\n",
    "        self.conv2 = nn.Conv2d(20, 1, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return self.conv2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules can be used as a submodule of another module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model1 = Model()\n",
    "        self.model2 = Model()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model1(1)\n",
    "        return self.model2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential is a module that contains other modules and applies them in sequence to produce its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    SuperModel(), # Note that we do not use F.ReLU here, since we need a module.\n",
    "    SuperModel()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning XOR\n",
    "\n",
    "Note! It is VERY important that the target vector has shape (N, 1) and not (N,). If the target vector has shape (N,), the loss function will not work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
    "target = torch.tensor([0., 1., 1., 0.]).reshape(4,1)\n",
    "\n",
    "# Let's define a very small neural network.\n",
    "perceptron = nn.Sequential(\n",
    "    nn.Linear(2, 2),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(2, 1),\n",
    ")\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "# Let's do a forward pass on this fake data:\n",
    "pred = perceptron(data)\n",
    "\n",
    "# Let's calculate the loss\n",
    "loss = loss_func(pred, target) \n",
    "\n",
    "gradient_before = next(perceptron.parameters()).grad\n",
    "\n",
    "# Now we must do some backwards propagation\n",
    "loss.backward() # This stores the gradient for every model param in the parameters .grad attribute\n",
    "\n",
    "gradient_after = next(perceptron.parameters()).grad # Now we have a gradient!\n",
    "\n",
    "# Now, we must load an optimizer. Most commonly, this is Stochastic Gradient Descent (SGD)\n",
    "optim = torch.optim.Adam(perceptron.parameters(), lr = 0.03)\n",
    "optim.step()\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this in a loop until our loss is less than 0.1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "loss = 1\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "for iterations in tqdm(range(1000)):\n",
    "    pred = perceptron(data)\n",
    "    loss = loss_func(pred, target)\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    if iterations % 10 == 0:\n",
    "        losses.append(loss.item())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use .apply() to apply a function to a model. This is useful for things like weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "perceptron.apply(reset_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bee7b5fc7d362c8ce9bc17001a34db42aeff7681f988faa8277ea7ddd77a43cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
