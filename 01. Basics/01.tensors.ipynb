{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Tensors\n",
    "For the full documentation, see [here](https://pytorch.org/docs/stable/tensors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Tensor has a _torch.dtype_, a _torch.device_ and _torch.layout_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(3)\n",
    "print(t.dtype)\n",
    "print(t.device)\n",
    "print(t.layout) # Layout is either strided (dense) or sparse_coo (sparse). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, -1],\n",
      "        [ 2,  1]])\n",
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[ 2, -2],\n",
      "        [ 4,  2]])\n",
      "tensor([[-1, -2],\n",
      "        [ 1, -4]])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Tensor can be constructed from a list:\n",
    "classic_list = [[1, -1], [2, 1]]\n",
    "\n",
    "t1 = torch.tensor(classic_list)\n",
    "t2 = torch.tensor(np.array(classic_list))\n",
    "\n",
    "print(t1)\n",
    "print(t1 == t2) # Compare values elementwise\n",
    "print(t1 * 2) # Scalar multiplication\n",
    "print(t1 @ (t1 - 1)) # Matrix multiplication using @\n",
    "\n",
    "single = torch.tensor(3)\n",
    "print(single.item()) # We can retrieve single tensor value using .item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are similar to NumPy's ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing. Most of the operations that can be performed on NumPy arrays can also be performed on Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19],\n",
      "        [20, 21, 22, 23],\n",
      "        [24, 25, 26, 27],\n",
      "        [28, 29, 30, 31],\n",
      "        [32, 33, 34, 35],\n",
      "        [36, 37, 38, 39]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "         36, 37, 38, 39]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(40)\n",
    "b = torch.reshape(a, (8,5)) # Reshape changes the dimensions of the tensor.\n",
    "c = torch.reshape(b, (10, 4)) # Note that the product of the dimension must equal the original count.\n",
    "print(c)\n",
    "\n",
    "matmul = c.T @ c # We can take the transpose of a tensor using T\n",
    "\n",
    "# Many functions on torch also work directly on the tensor:\n",
    "d = a.reshape((40,1))\n",
    "print(d.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('3.10.4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bee7b5fc7d362c8ce9bc17001a34db42aeff7681f988faa8277ea7ddd77a43cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
